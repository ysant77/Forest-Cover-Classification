# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Cr7aMLsB9yQu0J3IKk-lDw_021UODR3
"""

!pip install rasterio
!pip install tensorflow

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/DFC_Public_Dataset.zip

import matplotlib.pyplot as plt
import numpy as np
import rasterio
import os
import seaborn as sns

base_dir_name = '/content/DFC_Public_Dataset'
all_files = os.listdir(base_dir_name)
season_dirs = [file_name for file_name in all_files if os.path.isdir('{}/{}'.format(base_dir_name, file_name))]

print(season_dirs)

dfc_labels = []
s2_images = []

for season_dir in season_dirs:
  season_dir_path = '{}/{}'.format(base_dir_name, season_dir)
  sub_dirs = os.listdir(season_dir_path)
  sub_dirs = [sub_dir for sub_dir in sub_dirs if sub_dir.startswith("dfc") or sub_dir.startswith("s2")]
  for sub_dir in sub_dirs:
    sub_dir_path = "{}/{}".format(season_dir_path, sub_dir)
    files = os.listdir(sub_dir_path)
    files = ['{}/{}'.format(sub_dir_path, filename) for filename in files]
    if sub_dir.startswith("dfc"):
      dfc_labels.extend(files)
    else:
      s2_images.extend(files)

s2_images.sort()
dfc_labels.sort()

print(s2_images)
print(dfc_labels)

from tensorflow.keras import layers, Model
from tensorflow.keras.optimizers import Adam


def build_unet(input_shape, num_classes, weights_path=None):

    # Input layer
    inputs = layers.Input(input_shape)

    # Downsampling
    conv1, pool1 = create_conv_pool(inputs, 64)
    conv2, pool2 = create_conv_pool(pool1, 128)
    conv3, pool3 = create_conv_pool(pool2, 256)
    conv4, pool4 = create_conv_pool(pool3, 512)

    # Middle
    conv_middle = create_conv(pool4, 1024)

    # Upsampling
    up7 = layers.UpSampling2D(size=(2, 2))(conv_middle)
    up7 = layers.concatenate([up7, conv4])
    conv7 = create_conv(up7, 512)

    up8 = layers.UpSampling2D(size=(2, 2))(conv7)
    up8 = layers.concatenate([up8, conv3])
    conv8 = create_conv(up8, 256)

    up9 = layers.UpSampling2D(size=(2, 2))(conv8)
    up9 = layers.concatenate([up9, conv2])
    conv9 = create_conv(up9, 128)

    up10 = layers.UpSampling2D(size=(2, 2))(conv9)
    up10 = layers.concatenate([up10, conv1])
    conv10 = create_conv(up10, 64)

    # Output layer: Note the num_classes and 'softmax' activation
    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv10)

    model = Model(inputs=inputs, outputs=outputs)
    if weights_path is not None:
        model.load_weights(weights_path)

    return model

# Helper functions to create the convolution and pooling layers
def create_conv(input, filters, kernel_size=(3,3), padding="same", activation="relu"):
    conv = layers.Conv2D(filters, kernel_size, padding=padding, kernel_initializer="he_normal")(input)
    conv = layers.BatchNormalization()(conv)
    conv = layers.Activation(activation)(conv)
    conv = layers.Dropout(0.2)(conv)  # Dropout layer
    conv = layers.Conv2D(filters, kernel_size, padding=padding, kernel_initializer="he_normal")(conv)
    conv = layers.BatchNormalization()(conv)
    conv = layers.Activation(activation)(conv)
    conv = layers.Dropout(0.2)(conv)  # Dropout layer
    return conv


def create_conv_pool(input, filters, kernel_size=(3,3), padding="same", activation="relu"):
    conv = create_conv(input, filters, kernel_size, padding, activation)
    pool = layers.MaxPooling2D(pool_size=(2, 2))(conv)
    return conv, pool

# Build the model
input_shape = (256, 256, 2)  # Example input shape. Modify as needed.
num_classes = 6  # Example number of classes. Modify as needed.
model = build_unet(input_shape, num_classes)
opt = Adam(learning_rate=0.001, clipnorm=1.0)
model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

def pre_process_labels(label):
  label[label==1] = 0 # snow/ice to others
  label[(label==2) | (label==4) | (label==6)] = 1 # shrubland, savanna, grassland, croplands
  label[label==5] = 2 # wetlands
  label[label==7] = 3 # urban
  label[label==9] = 4 # urban
  label[label==10] = 5
  return label

# def pre_process_labels(label):
#     label = tf.where(label == tf.constant(1, dtype=tf.int16), tf.constant(0, dtype=tf.int16), label)  # snow/ice to others
#     conditions = (label == tf.constant(2, dtype=tf.int16)) | (label == tf.constant(4, dtype=tf.int16)) | (label == tf.constant(6, dtype=tf.int16))
#     label = tf.where(conditions, tf.constant(1, dtype=tf.int16), label)  # shrubland, savanna, grassland, croplands
#     label = tf.where(label == tf.constant(5, dtype=tf.int16), tf.constant(2, dtype=tf.int16), label)  # wetlands
#     label = tf.where(label == tf.constant(7, dtype=tf.int16), tf.constant(3, dtype=tf.int16), label)  # urban
#     label = tf.where(label == tf.constant(0, dtype=tf.int16), tf.constant(4, dtype=tf.int16), label)  # [A label, might be forest or something else]
#     label = tf.where(label == tf.constant(10, dtype=tf.int16), tf.constant(5, dtype=tf.int16), label)  # [Another label]
#     return label

import tensorflow as tf

X_train, X_test, y_train, y_test = train_test_split(s2_images, dfc_labels, test_size=0.2, random_state=42)

class SentinelLoaderTF:
    @staticmethod
    def preprocess_image(file_path):
        file_path = file_path.numpy().decode('utf-8')

        with rasterio.open(file_path) as src:
            red_band = src.read(4)  # B4
            nir_band = src.read(8)  # B8

        # Calculate NDVI
        ndvi = (nir_band - red_band) / (nir_band + red_band + 1e-8)

        # Normalization (assuming int16, so -32768 to 32767)
        nir_band = (nir_band - nir_band.min() ) / (nir_band.max() - nir_band.min())
        ndvi = (ndvi + 1) / 2  # Scale NDVI to [0, 1] as it's originally in [-1, 1]

        # Stacking
        ndvi = ndvi.astype(np.float32)
        nir_band = nir_band.astype(np.float32)
        return np.dstack((nir_band, ndvi))

    @staticmethod
    @tf.function
    def tf_preprocess_image(file_path):
        [image,] = tf.py_function(SentinelLoaderTF.preprocess_image, [file_path], [tf.float32])
        #image.set_shape([256, 256, 2])
        return image

    @staticmethod
    def load_label(file_path):
        file_path = file_path.numpy().decode('utf-8')
        with rasterio.open(file_path) as label_dataset:
            label = label_dataset.read(1)
        label = pre_process_labels(label)
        return label

    @staticmethod
    @tf.function
    def tf_load_label(file_path):
        [label,] = tf.py_function(SentinelLoaderTF.load_label, [file_path], [tf.int16])
        #label.set_shape([256, 256])
        return label

def create_dataset(image_files, label_files, batch_size=8):
    image_dataset = tf.data.Dataset.from_tensor_slices(image_files)
    label_dataset = tf.data.Dataset.from_tensor_slices(label_files)

    image_dataset = image_dataset.map(SentinelLoaderTF.tf_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
    label_dataset = label_dataset.map(SentinelLoaderTF.tf_load_label, num_parallel_calls=tf.data.AUTOTUNE)

    dataset = tf.data.Dataset.zip((image_dataset, label_dataset))
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)

    return dataset

tf.data.experimental.enable_debug_mode()

train_dataset = create_dataset(X_train, y_train)
val_dataset = create_dataset(X_test, y_test)

for images, labels in train_dataset.take(1):
    print(images.shape, images.dtype)
    print(labels.shape, labels.dtype)

tf.config.run_functions_eagerly(True)

unique_labels = set()

for _, labels in train_dataset:
    unique_in_batch = np.unique(labels.numpy())
    unique_labels.update(unique_in_batch)

print(unique_labels)

unique_labels1 = set()

for _, labels in val_dataset:
    unique_in_batch = np.unique(labels.numpy())
    unique_labels1.update(unique_in_batch)

print(unique_labels1)



from tensorflow.keras.callbacks import ModelCheckpoint
checkpoint_cb = ModelCheckpoint(f'/content/drive/MyDrive/unet_model_weights_100.h5', save_best_only=True, monitor='val_loss')

model.fit(train_dataset, validation_data=val_dataset, callbacks=[checkpoint_cb], epochs=100)



